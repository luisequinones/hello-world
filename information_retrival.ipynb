{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "148QXclOyERN2cB2262g4WSt4ZbpD2w3C",
      "authorship_tag": "ABX9TyOfoBj5Mls0kZ0AT7Ikb9ep",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luisequinones/hello-world/blob/master/information_retrival.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#installacion\n",
        "!pip install unidecode\n",
        "!pip install text_normalizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YW2RNCxUaPHs",
        "outputId": "da37e254-321a-47c0-b181-cda75f0be9ee"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting text_normalizer\n",
            "  Downloading text-normalizer-0.1.3.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from text_normalizer) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->text_normalizer) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->text_normalizer) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->text_normalizer) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->text_normalizer) (1.16.0)\n",
            "Building wheels for collected packages: text_normalizer\n",
            "  Building wheel for text_normalizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for text_normalizer: filename=text_normalizer-0.1.3-cp310-cp310-linux_x86_64.whl size=204399 sha256=874c3b84a8d279320329af76c4fe39c78ae2c268fa562e6eba7c6d2acf220003\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/9f/80/4a4e7d2d6f6fc35b19993353c2c8f1f7ac48ac29c826d2e676\n",
            "Successfully built text_normalizer\n",
            "Installing collected packages: text_normalizer\n",
            "Successfully installed text_normalizer-0.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNHlOmCIk4UL",
        "outputId": "9195f649-bded-40e7-f15b-72406af51213"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "#Importacion\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import text_normalizer as tn\n",
        "import nltk\n",
        "import textblob\n",
        "import json\n",
        "import urllib.request\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "import nltk\n",
        "import unidecode\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import spacy\n",
        "import string\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#funcion lectura\n",
        "def load_data(path):\n",
        "    import os\n",
        "    \n",
        "    #_____________ Read data from CISI.ALL file and store in dictinary ________________\n",
        "    \n",
        "    with open(os.path.join(path, 'CISI.ALL')) as f:\n",
        "        lines = \"\"\n",
        "        for l in f.readlines():\n",
        "            lines += \"\\n\" + l.strip() if l.startswith(\".\") else \" \" + l.strip()\n",
        "        lines = lines.lstrip(\"\\n\").split(\"\\n\")\n",
        " \n",
        "    doc_set = {}\n",
        "    doc_id = \"\"\n",
        "    doc_text = \"\"\n",
        "\n",
        "    for l in lines:\n",
        "        if l.startswith(\".I\"):\n",
        "            doc_id = l.split(\" \")[1].strip() \n",
        "        elif l.startswith(\".X\"):\n",
        "            doc_set[doc_id] = doc_text.lstrip(\" \")\n",
        "            doc_id = \"\"\n",
        "            doc_text = \"\"\n",
        "        else:\n",
        "            doc_text += l.strip()[3:] + \" \" \n",
        "\n",
        "    print(f\"Number of documents = {len(doc_set)}\")\n",
        "    print(doc_set[\"1\"]) \n",
        "    \n",
        "    \n",
        "    #_____________ Read data from CISI.QRY file and store in dictinary ________________\n",
        "    \n",
        "    with open(os.path.join(path, 'CISI.QRY')) as f:\n",
        "        lines = \"\"\n",
        "        for l in f.readlines():\n",
        "            lines += \"\\n\" + l.strip() if l.startswith(\".\") else \" \" + l.strip()\n",
        "        lines = lines.lstrip(\"\\n\").split(\"\\n\")\n",
        "          \n",
        "    qry_set = {}\n",
        "    qry_id = \"\"\n",
        "    for l in lines:\n",
        "        if l.startswith(\".I\"):\n",
        "            qry_id = l.split(\" \")[1].strip() \n",
        "        elif l.startswith(\".W\"):\n",
        "            qry_set[qry_id] = l.strip()[3:]\n",
        "            qry_id = \"\"\n",
        "\n",
        "    print(f\"\\n\\nNumber of queries = {len(qry_set)}\")    \n",
        "    print(qry_set[\"1\"]) \n",
        "    \n",
        "    \n",
        "    #_____________ Read data from CISI.REL file and store in dictinary ________________\n",
        "    \n",
        "    rel_set = {}\n",
        "    with open(os.path.join(path, 'CISI.REL')) as f:\n",
        "        for l in f.readlines():\n",
        "            qry_id = l.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[0] \n",
        "            doc_id = l.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[-1]\n",
        "\n",
        "            if qry_id in rel_set:\n",
        "                rel_set[qry_id].append(doc_id)\n",
        "            else:\n",
        "                rel_set[qry_id] = []\n",
        "                rel_set[qry_id].append(doc_id)\n",
        "\n",
        "    print(f\"\\n\\nNumber of mappings = {len(rel_set)}\")\n",
        "    print(rel_set[\"1\"]) \n",
        "    \n",
        "    doc_set = {int(id):doc for (id,doc) in doc_set.items()}\n",
        "    qry_set = {int(id):qry for (id,qry) in qry_set.items()}\n",
        "    rel_set = {int(qid):list(map(int, did_lst)) for (qid,did_lst) in rel_set.items()}\n",
        "    \n",
        "    return doc_set, qry_set, rel_set\n"
      ],
      "metadata": {
        "id": "SOCh_KdQlP-s"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lectura \n",
        "doc_set, qry_set, rel_set = load_data('/content/drive/MyDrive/Colab Notebooks/data/information_retrival')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqFhFYaSnwem",
        "outputId": "12b1e49c-e79c-48a8-da02-bcbe18dbc8e1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents = 1460\n",
            "18 Editions of the Dewey Decimal Classifications Comaromi, J.P. The present study is a history of the DEWEY Decimal Classification.  The first edition of the DDC was published in 1876, the eighteenth edition in 1971, and future editions will continue to appear as needed.  In spite of the DDC's long and healthy life, however, its full story has never been told.  There have been biographies of Dewey that briefly describe his system, but this is the first attempt to provide a detailed history of the work that more than any other has spurred the growth of librarianship in this country and abroad. \n",
            "\n",
            "\n",
            "Number of queries = 112\n",
            "What problems and concerns are there in making up descriptive titles? What difficulties are involved in automatically retrieving articles from approximate titles? What is the usual relevance of the content of articles to their titles?\n",
            "\n",
            "\n",
            "Number of mappings = 76\n",
            "['28', '35', '38', '42', '43', '52', '65', '76', '86', '150', '189', '192', '193', '195', '215', '269', '291', '320', '429', '465', '466', '482', '483', '510', '524', '541', '576', '582', '589', '603', '650', '680', '711', '722', '726', '783', '813', '820', '868', '869', '894', '1162', '1164', '1195', '1196', '1281']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#definicion Abreviaturas\n",
        "abreviaturas_jergas = {\n",
        "    \"ain't\": \"is not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"can't've\": \"cannot have\",\n",
        "    \"cause\": \"because\",\n",
        "    \"could've\": \"could have\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"couldn't've\": \"could not have\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"hadn't've\": \"had not have\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"he'd\": \"he would\",\n",
        "    \"he'd've\": \"he would have\",\n",
        "    \"he'll\": \"he will\",\n",
        "    \"he'll've\": \"he will have\",\n",
        "    \"he's\": \"he is\",\n",
        "    \"how'd\": \"how did\",\n",
        "    \"how'd'y\": \"how do you\",\n",
        "    \"how'll\": \"how will\",\n",
        "    \"how's\": \"how is\",\n",
        "    \"i'd\": \"I would\",\n",
        "    \"i'd've\": \"I would have\",\n",
        "    \"i'll\": \"I will\",\n",
        "    \"i'll've\": \"I will have\",\n",
        "    \"i'm\": \"I am\",\n",
        "    \"i've\": \"I have\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"it'd\": \"it would\",\n",
        "    \"it'd've\": \"it would have\",\n",
        "    \"it'll\": \"it will\",\n",
        "    \"it'll've\": \"it will have\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"let's\": \"let us\",\n",
        "    \"ma'am\": \"madam\",\n",
        "    \"mayn't\": \"may not\",\n",
        "    \"might've\": \"might have\",\n",
        "    \"mightn't\": \"might not\",\n",
        "    \"mightn't've\": \"might not have\",\n",
        "    \"must've\": \"must have\",\n",
        "    \"mustn't\": \"must not\",\n",
        "    \"mustn't've\": \"must not have\",\n",
        "    \"needn't\": \"need not\",\n",
        "    \"needn't've\": \"need not have\",\n",
        "    \"o'clock\": \"of the clock\",\n",
        "    \"oughtn't\": \"ought not\",\n",
        "    \"oughtn't've\": \"ought not have\",\n",
        "    \"shan't\": \"shall not\",\n",
        "    \"sha'n't\": \"shall not\",\n",
        "    \"shan't've\": \"shall not have\",\n",
        "    \"she'd\": \"she would\",\n",
        "    \"she'd've\": \"she would have\",\n",
        "    \"she'll\": \"she will\",\n",
        "    \"she'll've\": \"she will have\",\n",
        "    \"she's\": \"she is\",\n",
        "    \"should've\": \"should have\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"shouldn't've\": \"should not have\",\n",
        "    \"so've\": \"so have\",\n",
        "    \"so's\": \"so is\",\n",
        "    \"that'd\": \"that would\",\n",
        "    \"that'd've\": \"that would have\",\n",
        "    \"that's\": \"that is\",\n",
        "    \"there'd\": \"there would\",\n",
        "    \"there'd've\": \"there would have\",\n",
        "    \"there's\": \"there is\",\n",
        "    \"they'd\": \"they would\",\n",
        "    \"they'd've\": \"they would have\",\n",
        "    \"they'll\": \"they will\",\n",
        "    \"they'll've\": \"they will have\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"they've\": \"they have\",\n",
        "    \"to've\": \"to have\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"we'd\": \"we would\",\n",
        "    \"we'd've\": \"we would have\",\n",
        "    \"we'll\": \"we will\",\n",
        "    \"we'll've\": \"we will have\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"we've\": \"we have\",\n",
        "    \"weren't\": \"were not\",\n",
        "    \"what'll\": \"what will\",\n",
        "    \"what'll've\": \"what will have\",\n",
        "    \"what're\": \"what are\",\n",
        "    \"what's\": \"what is\",\n",
        "    \"what've\": \"what have\",\n",
        "    \"when's\": \"when is\",\n",
        "    \"when've\": \"when have\",\n",
        "    \"where'd\": \"where did\",\n",
        "    \"where's\": \"where is\",\n",
        "    \"where've\": \"where have\",\n",
        "    \"who'll\": \"who will\",\n",
        "    \"who'll've\": \"who will have\",\n",
        "    \"who's\": \"who is\",\n",
        "    \"who've\": \"who have\",\n",
        "    \"why's\": \"why is\",\n",
        "    \"why've\": \"why have\",\n",
        "    \"will've\": \"will have\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"won't've\": \"will not have\",\n",
        "    \"would've\": \"would have\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"wouldn't've\": \"would not have\",\n",
        "    \"y'all\": \"you all\",\n",
        "    \"y'all'd\": \"you all would\",\n",
        "    \"y'all'd've\": \"you all would have\",\n",
        "    \"y'all're\": \"you all are\",\n",
        "    \"y'all've\": \"you all have\",\n",
        "    \"you'd\": \"you would\",\n",
        "    \"you'd've\": \"you would have\",\n",
        "    \"you'll\": \"you will\",\n",
        "    \"you'll've\": \"you will have\",\n",
        "    \"you're\": \"you are\",\n",
        "    \"you've\": \"you have\",\n",
        "}"
      ],
      "metadata": {
        "id": "7qTuLnqQayIN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para limpiar el texto\n",
        "def clean_text_and_count_tokens(text):\n",
        "    # Eliminar hashtags, menciones y URLs\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "\n",
        "    # Poner el texto en minúsculas\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenizar el texto\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    num_tokens_initial = len(tokens)\n",
        "\n",
        "    # Sustitución de abreviaturas y jergas (agregar las abreviaturas y jergas relevantes)\n",
        "    tokens = [abreviaturas_jergas.get(t, t) for t in tokens]\n",
        "\n",
        "    # Eliminar stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    num_tokens_stopwords = len(tokens)\n",
        "\n",
        "    # Dejar solo palabras en inglés\n",
        "    tokens = [token for token in tokens if token.isalpha()]\n",
        "    num_tokens_alpha = len(tokens)\n",
        "\n",
        "    # Stemming\n",
        "    #stemmer = PorterStemmer()\n",
        "    #tokens_stemmed = [stemmer.stem(token) for token in tokens]\n",
        "    #num_tokens_stemmed = len(tokens_stemmed)\n",
        "\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "U0eDJPdYoBkO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#limpiar texto\n",
        "qry_set_clean= dict(zip(qry_set.keys(), map(clean_text_and_count_tokens, qry_set.values())))\n",
        "doc_set_clean= dict(zip(doc_set.keys(), map(clean_text_and_count_tokens, doc_set.values())))"
      ],
      "metadata": {
        "id": "SW765zu9S5OI"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qry_set_clean[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2TJYLa7c5lXw",
        "outputId": "e87b5c78-8734-435c-9e9f-7e0cd37059aa"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'problems concerns making descriptive titles difficulties involved automatically retrieving articles approximate titles usual relevance content articles titles'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qry_set[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "4AA0gANoeGvm",
        "outputId": "19c39783-9746-4887-b5b2-ba38a701f4c9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What problems and concerns are there in making up descriptive titles? What difficulties are involved in automatically retrieving articles from approximate titles? What is the usual relevance of the content of articles to their titles?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_set[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "ZUcN5Tm3fYWQ",
        "outputId": "d8f33249-2e44-46be-a7df-f9b31587036b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"18 Editions of the Dewey Decimal Classifications Comaromi, J.P. The present study is a history of the DEWEY Decimal Classification.  The first edition of the DDC was published in 1876, the eighteenth edition in 1971, and future editions will continue to appear as needed.  In spite of the DDC's long and healthy life, however, its full story has never been told.  There have been biographies of Dewey that briefly describe his system, but this is the first attempt to provide a detailed history of the work that more than any other has spurred the growth of librarianship in this country and abroad. \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_set_clean[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "sSxOHSaxfav0",
        "outputId": "5daddea6-e4f6-48a6-ae78-61676147be3b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'editions dewey decimal classifications comaromi jp present study history dewey decimal classification first edition ddc published eighteenth edition future editions continue appear needed spite ddcs long healthy life however full story never told biographies dewey briefly describe system first attempt provide detailed history work spurred growth librarianship country abroad'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ByswMwxOZDqv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}